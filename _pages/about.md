---
permalink: /
excerpt: "Master's Student in Computer Science | Medical AI Research"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<style>
  .jr-spark { opacity:0.9; margin:2px 0 10px; }
</style>

<div class="jr-spark">✦ ✦ ✦</div>

I am **Jirui Dai (戴纪瑞)**, a Master’s student in [Computer Science at Johns Hopkins University](https://www.cs.jhu.edu/).

Currently, I work as an **Assistant Researcher** (monthly stipend) in the [Cao Peng Group](https://yxy.njucm.edu.cn/2022/1026/c5740a108197/page.htm) at Nanjing University of Chinese Medicine, advised by Postdoctoral Researcher **Zhi Liu**. I am also involved in collaborative research with multiple medical institutions (including Peking Union Medical College Hospital, Dongfang Hospital of Beijing University of Chinese Medicine, and Gulou Hospital of Traditional Chinese Medicine of Beijing et al).

My interests sit at the intersection of **Medical AI** and **Reinforcement Learning**, with a focus on:
- **LLM、MLLM & Agent for clinical practice:** turning expert knowledge and real-world medical signals into reliable, usable systems.
- **RL for foundation models:** exploring whether reinforcement learning can expand model capabilities beyond “alignment” toward **broader, more reliable solution spaces**.

**I’m actively looking to collaborate** with researchers working on medical AI, multimodal learning, or RL for foundation models.  
You can find my CV [here](/assets/CV-Jirui_Dai.pdf). CV information before 01/15/2026.

## Research Taste
I’ve become more optimistic about the possibility of **AGI** as I learn more about modern AI systems — and at the same time, more cautious about the **hype cycles** around them. Progress is real, but the trajectory is hard to forecast, and many “confident narratives” dissolve under stress testing.

Where I feel the most grounded optimism is **healthcare**. Medicine suffers from structural scarcity: limited expert time, uneven access, and high cognitive load. Carefully designed medical AI can help scale expertise, improve triage and documentation, and make high-quality guidance more accessible — as long as we treat evaluation, safety, and distribution shift as first-class citizens.

On the methods side, I have a persistent curiosity about **reinforcement learning**. RL is imperfect and sometimes unstable, but it captures something deeply human: in real life, better outcomes often require navigating a long chain of trade-offs, and the “reward” is rarely explicit. I suspect there exists a hidden structure of incentives that can guide foundation models toward a **wider subset of outputs** — richer strategies, more robust reasoning patterns, and more creative problem solving — instead of collapsing into a narrow, overly safe, overly similar response manifold.

By participating in more projects and gaining a deeper understanding of AI in the future, I will continuously improve my research skills and judgment.

<div class="jr-spark">✦ ✦ ✦</div>
