---
permalink: /
excerpt: "Master's Student in Computer Science | Medical AI Research"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<style>
  .jr-hero {
    border: 1px solid rgba(127,127,127,0.25);
    border-radius: 16px;
    padding: 14px 16px;
    margin: 10px 0 16px;
    background: rgba(127,127,127,0.04);
    overflow: hidden;
    position: relative;
  }
  .jr-hero:before {
    content: "";
    position: absolute;
    inset: 0;
    background:
      radial-gradient(circle at 12% 30%, rgba(127,127,127,0.18), transparent 40%),
      radial-gradient(circle at 75% 20%, rgba(127,127,127,0.14), transparent 45%),
      radial-gradient(circle at 60% 85%, rgba(127,127,127,0.12), transparent 55%);
    pointer-events: none;
  }
  .jr-hero-inner {
    position: relative;
    display: flex;
    justify-content: space-between;
    align-items: center;
    gap: 12px;
    flex-wrap: wrap;
  }
  .jr-badges { display:flex; gap:8px; flex-wrap:wrap; }
  .jr-badge {
    display:inline-block;
    padding: 4px 10px;
    border-radius: 999px;
    border: 1px solid rgba(127,127,127,0.25);
    background: rgba(127,127,127,0.06);
    font-size: 0.92em;
    white-space: nowrap;
  }
  .jr-card {
    border: 1px solid rgba(127,127,127,0.25);
    border-radius: 16px;
    padding: 16px 18px;
    background: rgba(127,127,127,0.04);
    margin: 14px 0;
  }
  .jr-divider {
    height: 1px;
    background: rgba(127,127,127,0.25);
    margin: 18px 0;
  }
</style>

<div class="jr-hero">
  <div class="jr-hero-inner">
    <div style="font-weight:800; font-size:1.05em;">üß≠ Research Snapshot</div>
    <div class="jr-badges">
      <span class="jr-badge">üè• Medical AI</span>
      <span class="jr-badge">üß† Reinforcement Learning</span>
      <span class="jr-badge">üñºÔ∏è Multimodal</span>
    </div>
  </div>
</div>

<div class="jr-card">
I am **Jirui Dai (Êà¥Á∫™Áëû)**, a Master‚Äôs student in [Computer Science at Johns Hopkins University](https://www.cs.jhu.edu/).

Currently, I work as an **Assistant Researcher** (monthly stipend) in the [Cao Peng Group](https://yxy.njucm.edu.cn/2022/1026/c5740a108197/page.htm) at Nanjing University of Chinese Medicine, advised by Postdoctoral Researcher **Zhi Liu**. I am also involved in collaborative research with multiple medical institutions (including **Peking Union Medical College Hospital**, **Dongfang Hospital of Beijing University of Chinese Medicine**, and **Gulou Hospital of Traditional Chinese Medicine of Beijing**).
My interests sit at the intersection of **Medical AI** and **Reinforcement Learning**, with a focus on:
- **LLMs & multimodal models for clinical practice:** turning expert knowledge and real-world medical signals into reliable, usable systems.
- **RL for foundation models:** exploring whether reinforcement learning can expand model capabilities beyond ‚Äúpolite alignment‚Äù toward **broader, more reliable solution spaces**.
**I‚Äôm actively looking to collaborate** with researchers working on medical AI, multimodal learning, or RL for foundation models.
You can find my CV [here](/assets/CV-Jirui_Dai.pdf').CV information before 12/15/2025.
</div>

<div class="jr-divider"></div>

<div class="jr-card">
## Research Taste
I‚Äôve become more optimistic about the possibility of **AGI** as I learn more about modern AI systems ‚Äî and at the same time, more cautious about the **hype cycles** around them. Progress is real, but the trajectory is hard to forecast, and many ‚Äúconfident narratives‚Äù dissolve under stress testing.

Where I feel the most grounded optimism is **healthcare**. Medicine suffers from structural scarcity: limited expert time, uneven access, and high cognitive load. Carefully designed medical AI can help scale expertise, improve triage and documentation, and make high-quality guidance more accessible ‚Äî as long as we treat evaluation, safety, and distribution shift as first-class citizens.

On the methods side, I have a persistent curiosity about **reinforcement learning**. RL is imperfect and sometimes unstable, but it captures something deeply human: in real life, better outcomes often require navigating a long chain of trade-offs, and the ‚Äúreward‚Äù is rarely explicit. I suspect there exists a hidden structure of incentives that can guide foundation models toward a **wider subset of outputs** ‚Äî richer strategies, more robust reasoning patterns, and more creative problem solving ‚Äî instead of collapsing into a narrow, overly safe, overly similar response manifold.

By participating in more projects and gaining a deeper understanding of AI in the future, I will continuously improve my research skills and judgment.
</div>
